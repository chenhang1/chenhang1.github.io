<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>scrapy</title>
      <link href="/2021/11/17/scrapy/"/>
      <url>/2021/11/17/scrapy/</url>
      
        <content type="html"><![CDATA[<h3 id="一-概念"><a href="#一-概念" class="headerlink" title="一. 概念"></a>一. 概念</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scrapy是一个python编写的开源网络爬虫框架</span></span><br></pre></td></tr></table></figure><span id="more"></span><h3 id="二-流程"><a href="#二-流程" class="headerlink" title="二. 流程"></a>二. 流程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.爬虫中起始的url构造成的request对象---&gt;爬虫中间件----&gt;引擎----&gt;调度器存放</span></span><br><span class="line"><span class="comment"># 2. 调度器中的request---&gt;引擎---&gt;下载中间件---&gt;下载器</span></span><br><span class="line"><span class="comment"># 3.下载器发送请求,获取response响应---&gt;下载中间件---&gt;引擎---&gt;爬虫中间件---&gt;爬虫</span></span><br><span class="line"><span class="comment"># 4.如果获取到新的url地址,则组装成request对象---&gt;爬虫中间件---&gt;引擎---&gt;调度器,重复2,3步</span></span><br><span class="line"><span class="comment"># 5.如果获取的是爬虫数据---&gt;引擎---&gt;管道处理和保存数据</span></span><br><span class="line"></span><br><span class="line">注意: <span class="number">1.</span> 调度器模块,下载器模块,爬虫模块,管道处理数据模块都是相互独立的,其返回都是引擎</span><br><span class="line">     <span class="number">2.</span>request对象:由url method post_data headers等构成  </span><br><span class="line">     <span class="number">3.</span> response响应对象: 由url body status headers等构成</span><br><span class="line">     <span class="number">4.</span> item数据对象: 本质是字典</span><br><span class="line">       </span><br></pre></td></tr></table></figure><p><img src="scrapy_1.png"></p><h3 id="三-各模块的作用"><a href="#三-各模块的作用" class="headerlink" title="三. 各模块的作用"></a>三. 各模块的作用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scrapy Engine(引擎) : 总指挥,数据和信号的传递.框架已经实现</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Scheduler(调度器) : 任务队列,存放request对象.框架已经实现</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Downloader(下载器): 发送requests请求,获取相关信息 框架已经实现</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Spider(爬虫) : 处理response,解析数据/提取新的url 需要手写实现</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Item Pipline(管道): 处理数据,比如保存  需要手写实现</span></span><br><span class="line"><span class="comment"># DownloaderMiddlewares(下载中间件) : 自定义下载扩展,比如设置代理  一般不需要手写</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># SpiderMiddlewaresSpider(爬虫中间件): 自定义requests请求和进行response过滤  一般不需要手写</span></span><br></pre></td></tr></table></figure><h3 id="四-使用scrapy框架"><a href="#四-使用scrapy框架" class="headerlink" title="四. 使用scrapy框架"></a>四. 使用scrapy框架</h3><ol><li><p>安装</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install scrapy</span><br></pre></td></tr></table></figure><p>2.创建项目</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject 项目名</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建的各文件的作用</span></span><br><span class="line">items.py  需要爬取的内容</span><br><span class="line">middlewares.py 自定义中间件</span><br><span class="line">pipelines.py 管道 清洗保存数据</span><br><span class="line">spiders  文件夹存放爬虫程序, 提取解析数据</span><br><span class="line">settings.py 设置文件</span><br><span class="line">scrapy.cfg 项目配置文件</span><br></pre></td></tr></table></figure><p>3.建模</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">在item.py中提前规划好哪些字段需要抓取,能够让别人清楚自己的爬虫是在抓取什么</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">name = scrapy.Filed()  # 姓名</span></span><br><span class="line"><span class="string">title = scrapy.Filed() # 标题</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>4.创建爬虫</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd 项目  <span class="comment"># 在项目下</span></span><br><span class="line">scrapy genspider 爬虫名 爬取网站域名</span><br><span class="line"></span><br><span class="line"><span class="comment"># 爬虫文件中生成的内容</span></span><br><span class="line">三个参数: name 爬虫名</span><br><span class="line">         allowed_domains 允许爬取的网站域名</span><br><span class="line">         start_urls  设置起始url,会自动生成并发送请求</span><br><span class="line">一个方法: parse:解析数据方法,通常用于起始url对应的响应的解析</span><br><span class="line">         </span><br></pre></td></tr></table></figure><p>5.完成爬虫</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改起始的url</span></span><br><span class="line"><span class="comment"># 检查修改允许的域名</span></span><br><span class="line"><span class="comment">#在parse方法中实现爬取逻辑运行爬虫</span></span><br><span class="line"><span class="number">1.</span> 在提取数据时使用response.xpath(),其得到是一个选择器对象,一般操作和提取列表中某个元素一样,然后使用extract()方法返回字符串的列表.</span><br><span class="line">如果列表中只有一个数据可以使extract_first()</span><br><span class="line">     <span class="comment"># 例如: response.xpath(&quot;/div[@class=&#x27;tes_con&#x27;]&quot;)[0].extract()</span></span><br><span class="line">         <span class="number">2.</span>在返回数据给引擎时,一般使用<span class="keyword">yield</span> 关键字</span><br><span class="line">         使用生成器,可以将返回的数据读到内存中,不会造成内存占用过高,同时对翻页操作也方便.</span><br><span class="line">         <span class="number">3.</span> 在程序中必须有parse方法</span><br><span class="line">         <span class="number">4.</span>在需要抓取的url,必须是allowed_domains中允许的,不然会被过滤掉</span><br></pre></td></tr></table></figure><p>6.保存数据</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在pipelines.py 文件中定义对数据的操作</span></span><br><span class="line"> <span class="number">1.</span> 在process_item()方法中,item参数接收的是引擎传过来的解析数据(字典类型),可以直接打印,但是如果建模后,item是一个对象,需要将其强制转换<span class="built_in">dict</span>(item),这种操作只能在scrapy框架中使用</span><br><span class="line"></span><br><span class="line">     <span class="number">2.</span>将数据处理写入文件中,首先将字典数据序列化成json字符串,同时序列化时中文默认使用<span class="built_in">ascii</span>编码.想输出真正的中文需要设置ensure_ascii=<span class="literal">False</span> ,最后将数据写入文件</span><br><span class="line"> </span><br><span class="line"> <span class="number">3.</span>默认使用完管道之后需要将数据返回给引擎  <span class="comment"># return item</span></span><br><span class="line"> <span class="number">4.</span> 在setting.py中ITEM_PIPLINES项中注册启用管道</span><br><span class="line"> 权重越小越优先</span><br><span class="line"> ```</span><br></pre></td></tr></table></figure><p><img src="scrapy_2.png"></p><p>7.运行爬虫</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在项目目录下</span></span><br><span class="line"> scrapy crawl 爬虫名</span><br></pre></td></tr></table></figure><h3 id="五-数据的相关处理问题"><a href="#五-数据的相关处理问题" class="headerlink" title="五. 数据的相关处理问题"></a>五. 数据的相关处理问题</h3><ol><li>翻页请求</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># 在requests模块中实现翻页方式:</span></span><br><span class="line">先找到下一页的URL地址,再调用requests.get(url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在scrapy框架中实现翻页方式:</span></span><br><span class="line">先获取下一页的URL地址,再通过scrapy.Request(url,callback)得到一个request对象,最后通过<span class="keyword">yield</span>关键字把这个对象交给引擎</span><br><span class="line">   <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">   # 获取下一页URL地址</span></span><br><span class="line"><span class="string">   part_url  = response.xpath(&quot;/html/body/div[2]/div[2]/div[2]/div/a[last()]/@href&quot;).extract_first()</span></span><br><span class="line"><span class="string">   # 判断终止条件</span></span><br><span class="line"><span class="string">   if part_url != &#x27;javascript:void(0)&#x27;:</span></span><br><span class="line"><span class="string">      # 由于获取的地址是相对路径需要拼接</span></span><br><span class="line"><span class="string">      #response.urljoin() : 将url的域名部分拼接获取的相对路径</span></span><br><span class="line"><span class="string">      next_url = response.urljoin(part_url)</span></span><br><span class="line"><span class="string">      # 构建请求对象,并返回给引擎</span></span><br><span class="line"><span class="string">      yield scrapy.Request(</span></span><br><span class="line"><span class="string">      url = next_url,</span></span><br><span class="line"><span class="string">      callback = self.parse</span></span><br><span class="line"><span class="string">      )</span></span><br><span class="line"><span class="string">       </span></span><br><span class="line"><span class="string">   &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>scrapy.Request的参数</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方括号为可选参数scrapy.Request(url[,callback,method=&quot;GET&quot;,headers,body,cookies,meta,dont_filter=False])</span></span><br><span class="line">  callback：表示当前的url的响应交给哪个函数去处理</span><br><span class="line">  meta：实现数据在不同的解析函数中传递,常用在一个数据分散在不同页面结构中的情况.</span><br><span class="line">  dont_filter:默认会过滤请求的url地址，即请求过的url地址不会继续被请求，对需要重复请求的url地址可以把它设置为Ture，比如贴吧的翻页请求，页面的数据总是在变化;start_urls中的地址会被反复请求，否则程序不会启动</span><br><span class="line">  method: 指定POST或GET请求</span><br><span class="line">  headers: 接收一个字典.其中不包括cookies</span><br><span class="line">  cookies: 接收一个字典,专门放置cookies </span><br><span class="line">  body:接收json字符串,为POST的数据,发送payload_post请求时使用</span><br><span class="line"> </span><br></pre></td></tr></table></figure><ol start="3"><li><p>模拟登陆</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">实现模拟登陆的方式:</span><br><span class="line">    <span class="number">1.</span> requests模块: </span><br><span class="line">  携带cookies请求(适合cookies有效期长的)</span><br><span class="line">        获取url地址,发送post请求,存储cookie</span><br><span class="line">    <span class="number">2.</span> selenium模块: </span><br><span class="line">        找到对应的<span class="built_in">input</span>标签,输入文本点击登录</span><br><span class="line">    <span class="number">3.</span> scrapy 框架: 同requests</span><br><span class="line">        <span class="comment">#直接携带cookie实现过程: 由于模拟登录需要携带cookie,但是框架中的start_requests()方法没有实现,所以需要重构,获取到cookie</span></span><br><span class="line">        <span class="comment"># 通过post请求的过程: 找到请求登录的url地址,再分析请求体中参数的规律(这是最难的地方)最后模拟登陆</span></span><br><span class="line">      <span class="comment">#注意: </span></span><br><span class="line">    <span class="number">1.</span>scrapy中的cookie不能够放在headers中</span><br><span class="line">    <span class="number">2.</span>在运行时需要将setting中的ROBTS协议关闭,同时也要模拟请求头</span><br><span class="line">     <span class="number">3.</span>在settings.py中通过设置COOKIES_DEBUG=TRUE 能够在终端看到cookie的传递传递过程   </span><br></pre></td></tr></table></figure></li><li><p>管道使用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">当创建了两个爬虫后,如果对管道不进行处理,则会出现运行一个爬虫,会保存两份数据,所以需要通过spider参数的name属性进行一个判断.同时每次通过open_spider(self, spider)方法开启管道,</span><br><span class="line">close_spider(self, spider)关闭管道</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意点:</span></span><br><span class="line"><span class="number">1.</span> 使用之前需要在settings中开启</span><br><span class="line"><span class="number">2.</span> pipeline在setting中键表示位置(即pipeline在项目中的位置可以自定义)，值表示距离引擎的远近，越近数据会越先经过：**权重值小的优先执行**</span><br><span class="line"><span class="number">3.</span> 有多个pipeline的时候，process_item的方法必须<span class="keyword">return</span> item,否则后一个pipeline取到的数据为<span class="literal">None</span>值</span><br><span class="line"><span class="number">4.</span> pipeline中process_item的方法必须有，否则item没有办法接受和处理</span><br><span class="line"><span class="number">5.</span> process_item方法接受item和spider，其中spider表示当前传递item过来的spider</span><br><span class="line"><span class="number">6.</span> open_spider(spider) :能够在爬虫开启的时候执行一次</span><br><span class="line"><span class="number">7.</span> close_spider(spider) :能够在爬虫关闭的时候执行一次</span><br><span class="line"><span class="number">8.</span> 上述俩个方法经常用于爬虫和数据库的交互，在爬虫开启的时候建立和数据库的连接，在爬虫关闭的时候断开和数据库的连接</span><br></pre></td></tr></table></figure></li><li><p>crawlspider</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">crawlspider爬虫解决的需要处理很多url地址时,根据自己定义的规则构造requests请求,发送给引擎</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span>创建:scrapy genspider -t crawl 爬虫名 域名</span><br><span class="line"><span class="number">2.</span>参数:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    rules = (</span></span><br><span class="line"><span class="string">        Rule(LinkExtractor(allow=r&#x27;Items/&#x27;), callback=&#x27;parse_item&#x27;, follow=True),</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="number">1.</span> rules是一个元组或者是列表，包含的是Rule对象</span><br><span class="line">    <span class="number">2.</span> Rule表示规则，其中包含LinkExtractor,callback和follow等参数</span><br><span class="line">    <span class="number">3.</span> LinkExtractor:连接提取器，可以通过正则或者是xpath来进行url地址的匹配</span><br><span class="line">    <span class="number">4.</span> callback :表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数的处理</span><br><span class="line">    <span class="number">5.</span> follow：连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，<span class="literal">True</span>表示会，Flase表示不会</span><br><span class="line"><span class="number">3.</span>注意点:</span><br><span class="line">    <span class="number">1.</span> 除了用命令scrapy genspider -t crawl &lt;爬虫名&gt; &lt;allowed_domail&gt;创建一个crawlspider的模板，页可以手动创建</span><br><span class="line"><span class="number">2.</span> crawlspider中不能再有以parse为名的数据提取方法，该方法被crawlspider用来实现基础url提取等功能</span><br><span class="line"><span class="number">3.</span> Rule对象中LinkExtractor为固定参数，其他callback、follow为可选参数</span><br><span class="line"><span class="number">4.</span> 不指定callback且follow为<span class="literal">True</span>的情况下，满足rules中规则的url还会被继续提取和请求</span><br><span class="line"><span class="number">5.</span> 如果一个被提取的url满足多个Rule，那么会从rules中选择一个满足匹配条件的Rule执行</span><br></pre></td></tr></table></figure></li><li><p>中间件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> 类别:下载中间件 爬虫中间件</span><br><span class="line"><span class="number">2.</span> 作用: 预处理request 和response对象</span><br><span class="line">    ① 对header以及cookie进行处理</span><br><span class="line">    ② 使用代理ip</span><br><span class="line">    ③ 对请求进行定制化操作</span><br><span class="line"><span class="number">3.</span> 使用方法: </span><br><span class="line">    ①需要在Middlewares.py文件定义一个类,然后在setting中开启</span><br><span class="line">    ②两个方法: process_request(self,request,spider)</span><br><span class="line">              每个请求先经过下载中间件,该方法被调用.</span><br><span class="line">            a. 返回<span class="literal">None</span>值: 表示该request请求传递给下载器</span><br><span class="line">              或者返回给引擎传递给了权重低的process_request方法</span><br><span class="line">            b. 返回Response对象：不再请求，把response返回给引擎</span><br><span class="line">            c. 返回Request对象：把request对象通过引擎交给调度器，此时将不通过其他权重低的process_request方法  </span><br><span class="line">         process_response(self, request, response, spider)</span><br><span class="line">           下载器完成请求后,返回给引擎时被调用</span><br><span class="line">            a. 返回Resposne：通过引擎交给爬虫处理或交给权重更低的其他下载中间件的process_response方法</span><br><span class="line">   b. 返回Request对象：通过引擎交给调取器继续请求，此时将不通过其他权重低的process_request方法</span><br><span class="line">   <span class="number">4.</span>应用:</span><br><span class="line">    ① 实现随机User-Agent</span><br><span class="line">        ② 代理ip</span><br><span class="line">        免费代理ip</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        class ProxyMiddleware(object):</span></span><br><span class="line"><span class="string">    def process_request(self,request,spider):</span></span><br><span class="line"><span class="string">        # proxies可以在settings.py中，也可以来源于代理ip的webapi</span></span><br><span class="line"><span class="string">        # proxy = random.choice(proxies) </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        # 免费的会失效，报 111 connection refused 信息！重找一个代理ip再试</span></span><br><span class="line"><span class="string">        proxy = &#x27;https://1.71.188.37:3128&#x27; </span></span><br><span class="line"><span class="string">        request.meta[&#x27;proxy&#x27;] = proxy</span></span><br><span class="line"><span class="string">         return None # 可以不写return</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        收费代理ip</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        # 人民币玩家的代码(使用abuyun提供的代理ip)</span></span><br><span class="line"><span class="string">import base64</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># 代理隧道验证信息  这个是在那个网站上申请的</span></span><br><span class="line"><span class="string">proxyServer = &#x27;http://proxy.abuyun.com:9010&#x27; # 收费的代理ip服务器地址，这里是abuyun</span></span><br><span class="line"><span class="string">proxyUser = 用户名</span></span><br><span class="line"><span class="string">proxyPass = 密码</span></span><br><span class="line"><span class="string">proxyAuth = &quot;Basic &quot; + base64.b64encode(proxyUser + &quot;:&quot; + proxyPass)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class ProxyMiddleware(object):</span></span><br><span class="line"><span class="string">    def process_request(self, request, spider):</span></span><br><span class="line"><span class="string">        # 设置代理</span></span><br><span class="line"><span class="string">        request.meta[&quot;proxy&quot;] = proxyServer</span></span><br><span class="line"><span class="string">        # 设置认证</span></span><br><span class="line"><span class="string">        request.headers[&quot;Proxy-Authorization&quot;] = proxyAuth</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        检测代理ip</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        class ProxyMiddleware(object):</span></span><br><span class="line"><span class="string">    ......</span></span><br><span class="line"><span class="string">    def process_response(self, request, response, spider):</span></span><br><span class="line"><span class="string">        if response.status != &#x27;200&#x27;:</span></span><br><span class="line"><span class="string">            request.dont_filter = True # 重新发送的请求对象能够再次进入队列</span></span><br><span class="line"><span class="string">            return requst</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">   ③ 使用selenium 模拟登录</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="number">5.</span>注意点: </span><br><span class="line">     ①  需要在settings.py中开启中间件</span><br><span class="line">    例如:</span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line"><span class="string">&#x27;myspider.middlewares.UserAgentMiddleware&#x27;</span>: <span class="number">543</span>,</span><br><span class="line">&#125;</span><br><span class="line">    ②但在scrapy默认的情况下 两种中间件都在middlewares.py一个文件中</span><br><span class="line">    ③爬虫中间件使用方法和下载中间件相同，且功能重复，通常使用下载中间件</span><br><span class="line">    </span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Supervisor&amp; shadowsocks</title>
      <link href="/2021/11/16/Supervisor/"/>
      <url>/2021/11/16/Supervisor/</url>
      
        <content type="html"><![CDATA[<h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>supervisor是进程管理工具，其python3支持不好，须使用python2</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip install supervisor</span><br></pre></td></tr></table></figure><span id="more"></span><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>运行<strong>echo_supervisord_conf</strong>命令输出默认的配置项，可以如下操作将默认配置保存到文件中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo_supervisord_conf &gt; supervisord.conf</span><br></pre></td></tr></table></figure><p>vim 打开编辑supervisord.conf文件，修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[include]</span><br><span class="line">files = relative/directory/*.ini</span><br></pre></td></tr></table></figure><p>为</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[include]</span><br><span class="line">files = /etc/supervisor/*.conf</span><br></pre></td></tr></table></figure><p>include选项指明包含的其他配置文件。</p><p>将编辑后的supervisord.conf文件复制到/etc/目录下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo cp supervisord.conf /etc/</span><br></pre></td></tr></table></figure><p>然后我们在/etc目录下新建子目录supervisor（与配置文件里的选项相同），并在/etc/supervisor/中新建tuotiao管理的配置文件toutiao.conf。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">[inet_http_server]         ; inet (TCP) server disabled by default</span><br><span class="line">port=0.0.0.0:9001        ; ip_address:port specifier, *:port for all iface</span><br><span class="line">username=chen              ; default is no username (open server)</span><br><span class="line">password=123               ; default is no password (open server)</span><br><span class="line"></span><br><span class="line">[supervisorctl]</span><br><span class="line">serverurl=unix:///tmp/supervisor.sock ; use a unix:// URL  for a unix socket</span><br><span class="line">;serverurl=http://127.0.0.1:9001 ; use an http:// url to specify an inet socket</span><br><span class="line">;username=chris              ; should be same as in [*_http_server] if set</span><br><span class="line">;password=123                ; should be same as in [*_http_server] if set</span><br><span class="line">;prompt=mysupervisor         ; cmd line prompt (default &quot;supervisor&quot;)</span><br><span class="line">;history_file=~/.sc_history  ; use readline history if available</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[group:toutiao]</span><br><span class="line">programs=toutiao-app</span><br><span class="line"></span><br><span class="line">[program:toutiao-app]</span><br><span class="line">command=/home/python/scripts/toutiao_app.sh</span><br><span class="line">directory=/home/python/toutiao-backend</span><br><span class="line">user=python</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=false</span><br><span class="line">loglevel=info</span><br><span class="line">stopsignal=KILL</span><br><span class="line">stopasgroup=true</span><br><span class="line">killasgroup=true</span><br><span class="line"></span><br><span class="line">[program:im]</span><br><span class="line">command=/home/python/scripts/im.sh</span><br><span class="line">directory=/home/python/toutiao-backend</span><br><span class="line">user=python</span><br><span class="line">autorestart=true</span><br><span class="line">redirect_stderr=false</span><br><span class="line">loglevel=info</span><br><span class="line">stopsignal=KILL</span><br><span class="line">stopasgroup=true</span><br><span class="line">killasgroup=true</span><br><span class="line"></span><br><span class="line">[program:caipiao]</span><br><span class="line">command= nohup python3 app.py &gt; python.log 2&gt;&amp;1 &amp;  ; 被监控的进程启动命令</span><br><span class="line">directory=/root/caipiao/                ; 执行前要不要先cd到目录去，一般不用</span><br><span class="line">priority=1                    ;数字越高，优先级越高</span><br><span class="line">numprocs=1                    ; 启动几个进程</span><br><span class="line">autostart=true                ; 随着supervisord的启动而启动</span><br><span class="line">autorestart=true              ; 自动重启。。当然要选上了</span><br><span class="line">startretries=10               ; 启动失败时的最多重试次数</span><br><span class="line">exitcodes=0                   ; 正常退出代码（是说退出代码是这个时就不再重启了吗？待确定）</span><br><span class="line">stopsignal=KILL               ; 用来杀死进程的信号</span><br><span class="line">stopwaitsecs=10               ; 发送SIGKILL前的等待时间</span><br><span class="line">redirect_stderr=true          ; 重定向stderr到stdout</span><br><span class="line"></span><br><span class="line">[program:ss]</span><br><span class="line">command= nohup ssserver -c /etc/shadowsocks.json &amp;  ; 被监控的进程启动命令</span><br><span class="line">directory=/root/                ; 执行前要不要先cd到目录去，一般不用</span><br><span class="line">priority=1                    ;数字越高，优先级越高</span><br><span class="line">numprocs=1                    ; 启动几个进程</span><br><span class="line">autostart=true                ; 随着supervisord的启动而启动</span><br><span class="line">autorestart=true              ; 自动重启。。当然要选上了</span><br><span class="line">startretries=10               ; 启动失败时的最多重试次数</span><br><span class="line">exitcodes=0                   ; 正常退出代码（是说退出代码是这个时就不再重启了吗？待确定）</span><br><span class="line">stopsignal=KILL               ; 用来杀死进程的信号</span><br><span class="line">stopwaitsecs=10               ; 发送SIGKILL前的等待时间</span><br><span class="line">redirect_stderr=true          ; 重定向stderr到stdout</span><br><span class="line">stopasgroup=true              ;stop program之时一并stop其子进程</span><br><span class="line">killasgroup=true              ;在停止program子进程的时候，发送kill信号</span><br></pre></td></tr></table></figure><h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">supervisord -c /etc/supervisord.conf</span><br></pre></td></tr></table></figure><p>查看 supervisord 是否在运行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | grep supervisord</span><br></pre></td></tr></table></figure><h4 id="supervisorctl"><a href="#supervisorctl" class="headerlink" title="supervisorctl"></a>supervisorctl</h4><p>我们可以利用supervisorctl来管理supervisor。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">supervisorctl</span><br><span class="line"></span><br><span class="line">&gt; status    <span class="comment"># 查看程序状态</span></span><br><span class="line">&gt; start apscheduler  <span class="comment"># 启动 apscheduler 单一程序</span></span><br><span class="line">&gt; stop toutiao:*   <span class="comment"># 关闭 toutiao组 程序</span></span><br><span class="line">&gt; start toutiao:*  <span class="comment"># 启动 toutiao组 程序</span></span><br><span class="line">&gt; restart toutiao:*    <span class="comment"># 重启 toutiao组 程序</span></span><br><span class="line">&gt; update    ＃ 重启配置文件修改过的程序</span><br></pre></td></tr></table></figure><p>执行status命令时，显示如下信息说明程序运行正常：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">supervisor&gt; status</span><br><span class="line">toutiao:toutiao-app RUNNING pid 32091, uptime 00:00:02</span><br></pre></td></tr></table></figure><h2 id="shadowsocks"><a href="#shadowsocks" class="headerlink" title="shadowsocks"></a>shadowsocks</h2><h3 id="配置-1"><a href="#配置-1" class="headerlink" title="配置"></a>配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1. yum install python-setuptools &amp;&amp; easy_install pip</span><br><span class="line">2. pip install shadowsocks</span><br><span class="line"></span><br><span class="line">3. vi /etc/shadowsocks.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;server&quot;</span>: <span class="string">&quot;0.0.0.0&quot;</span>,</span><br><span class="line">  <span class="string">&quot;port_password&quot;</span>:&#123;</span><br><span class="line">   <span class="string">&quot;8886&quot;</span>:<span class="string">&quot;123456789chen&quot;</span>,</span><br><span class="line">   <span class="string">&quot;8887&quot;</span>:<span class="string">&quot;123456789chen&quot;</span></span><br><span class="line">   &#125;,</span><br><span class="line">  <span class="string">&quot;method&quot;</span>: <span class="string">&quot;aes-256-cfb&quot;</span>,</span><br><span class="line">  <span class="string">&quot;timeout&quot;</span>: 300,</span><br><span class="line">  <span class="string">&quot;mode&quot;</span>: <span class="string">&quot;tcp_and_udp&quot;</span>,</span><br><span class="line">  <span class="string">&quot;workers&quot;</span>:2</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="启动-1"><a href="#启动-1" class="headerlink" title="启动"></a>启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssserver -c /etc/shadowsocks.json</span><br><span class="line">ssserver -c /etc/shadowsocks.json -d start <span class="comment"># 后台运行</span></span><br><span class="line">ssserver -c /etc/shadowsocks.json -d stop <span class="comment">#停止</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 运维 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 代理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS7安装 redis</title>
      <link href="/2021/11/16/centos%E5%AE%89%E8%A3%85redis/"/>
      <url>/2021/11/16/centos%E5%AE%89%E8%A3%85redis/</url>
      
        <content type="html"><![CDATA[<p>方法一：源文件安装（推荐安装）</p><p>在CentOS和Red Hat系统中，首先添加EPEL仓库，然后更新yum源：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install epel-release</span><br><span class="line">sudo yum update</span><br></pre></td></tr></table></figure><span id="more"></span><p>然后安装Redis数据库：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install redis</span><br></pre></td></tr></table></figure><p>安装好后启动Redis服务即可：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start redis   #开启</span><br><span class="line">redis-cli shutdown  # 关闭</span><br></pre></td></tr></table></figure><p>这里同样可以使用redis-cli进入Redis命令行模式操作。</p><p>另外，为了可以使Redis能被远程连接，需要修改配置文件，路径为/etc/redis.conf</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/redis.conf</span><br></pre></td></tr></table></figure><p>需要修改的地方：</p><p>首先，注释这一行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#bind 127.0.0.1</span><br></pre></td></tr></table></figure><p>另外，推荐给Redis设置密码，取消注释这一行：</p><p>#requirepass foobared</p><p>foobared即当前密码，可以自行修改为</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requirepass 密码</span><br></pre></td></tr></table></figure><p>然后重启Redis服务，使用的命令如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart redis</span><br></pre></td></tr></table></figure><p>systemctl start redis.service #启动redis服务器</p><p>systemctl stop redis.service #停止redis服务器</p><p>systemctl restart redis.service #重新启动redis服务器</p><p>systemctl status redis.service #获取redis服务器的运行状态</p><p>systemctl enable redis.service #开机启动redis服务器</p><p>systemctl disable redis.service #开机禁用redis服务器</p><p> 方法二；压缩包安装</p><p><strong>一、安装</strong></p><p><strong>1、</strong>在官网下载tar.gz的安装包，或者通过wget的方式下载　　</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.redis.io/releases/redis-4.0.1.tar.gz</span><br></pre></td></tr></table></figure><p>***2、***解压</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf redis-4.0.1.tar.gz</span><br></pre></td></tr></table></figure><p>3、编译</p><p>通过make来编译，make是自动编译，会根据Makefile中描述的内容来进行编译。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd redis-4.0</span><br><span class="line">make</span><br></pre></td></tr></table></figure><p>可以看到在src目录下生成了几个新的文件。</p><p>4、安装</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make install</span><br></pre></td></tr></table></figure><p>实际上，就是将这个几个文件加到/usr/local/bin目录下去。这个目录在Path下面的话，就可以直接执行这几个命令了。</p><p>可以看到，这几个文件就已经被加载到bin目录下了</p><p>5、启动服务器，来看看安装是否成功。</p><p>使用redis-server命令。</p><p>再启动一个linux客户端，redis-cli打开客户端　　</p><p>6、关闭服务器</p><p>再启动一个linux客户端，通过server-cli shutdown来关闭服务器。</p><p><strong>二、配置</strong></p><p>1、设置redis服务器后台启动</p><p>前面在启动redis服务器后，都是在前台启动的，需要重新启动一个客户端来进行登陆操作。这样非常不方便，</p><p>所以，我们需要设置后台启动。</p><p>daemonize no</p><p>修改为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">daemonize yes</span><br></pre></td></tr></table></figure><p><strong>卸载redis</strong></p><p>卸载redis服务，只需把/usr/local/bin/目录下的redis删除即可</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /usr/local/bin/redis*</span><br></pre></td></tr></table></figure><p>甚至可以把解压包也删除掉</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /root/redis-stable</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> redis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CentOS7安装 Python3.7</title>
      <link href="/2021/11/15/CentOS7%E5%AE%89%E8%A3%85Python3.7%E5%8F%8A%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/"/>
      <url>/2021/11/15/CentOS7%E5%AE%89%E8%A3%85Python3.7%E5%8F%8A%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<h4 id="1-查看python2"><a href="#1-查看python2" class="headerlink" title="1.  查看python2"></a>1.  查看python2</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@lidan /]# whereis python</span><br><span class="line">python: /usr/bin/python /usr/bin/python2.7 /usr/bin/python.bak /usr/lib/python2.7 /usr/lib64/python2.7 /etc/python /usr/include/python2.7 /usr/share/man/man1/python.1.gz</span><br><span class="line">[root@lidan bin]# ll python*</span><br><span class="line">lrwxrwxrwx. 1 root root    9 5月  27 2016 python2 -&gt; python2.7</span><br><span class="line">-rwxr-xr-x. 1 root root 7136 11月 20 2015 python2.7</span><br><span class="line">lrwxrwxrwx. 1 root root    7 5月  27 2016 python.bak -&gt; python2</span><br></pre></td></tr></table></figure><span id="more"></span><h4 id="2-接下来我们要安装编译-Python3的相关包"><a href="#2-接下来我们要安装编译-Python3的相关包" class="headerlink" title="2. 接下来我们要安装编译 Python3的相关包"></a>2. 接下来我们要安装编译 Python3的相关包</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make libffi-devel</span><br></pre></td></tr></table></figure><p>这里面有一个包很关键<code>libffi-devel</code>，因为只有3.7才会用到这个包，如果不安装这个包的话，在 make 阶段会出现如下的报错：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ModuleNotFoundError: No module named &#x27;_ctypes&#x27;</span><br></pre></td></tr></table></figure><h4 id="3-安装pip"><a href="#3-安装pip" class="headerlink" title="3. 安装pip"></a>3. 安装pip</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#运行这个命令添加epel扩展源 </span><br><span class="line">yum -y install epel-release </span><br><span class="line">#安装pip </span><br><span class="line">yum install python-pip</span><br></pre></td></tr></table></figure><h4 id="4-python-安装wget"><a href="#4-python-安装wget" class="headerlink" title="4. python 安装wget"></a>4. python 安装wget</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install wget</span><br></pre></td></tr></table></figure><h4 id="5-下载-python3-7的源码包"><a href="#5-下载-python3-7的源码包" class="headerlink" title="5. 下载 python3.7的源码包"></a>5. 下载 python3.7的源码包</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget https://www.python.org/ftp/python/3.7.0/Python-3.7.0.tgz</span><br><span class="line">#解压缩</span><br><span class="line">tar -zxvf Python-3.7.0.tgz</span><br><span class="line"></span><br><span class="line">#进入解压后的目录，依次执行下面命令进行手动编译</span><br><span class="line">./configure prefix=/usr/local/python3 </span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure><p>如果最后没提示出错，就代表正确安装了，在/usr/local/目录下就会有python3目录</p><h4 id="6-添加软链接"><a href="#6-添加软链接" class="headerlink" title="6. 添加软链接"></a>6. 添加软链接</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#添加python3的软链接 </span><br><span class="line">ln -s /usr/local/python3/bin/python3.7 /usr/bin/python3.7 </span><br><span class="line">#添加 pip3 的软链接 </span><br><span class="line">ln -s /usr/local/python3/bin/pip3.7 /usr/bin/pip3.7</span><br><span class="line">#测试是否安装成功了 </span><br><span class="line">python -V</span><br></pre></td></tr></table></figure><h4 id="7-更改yum配置，因为其要用到python2才能执行，否则会导致yum不能正常使用（不管安装-python3的那个版本，都必须要做的）"><a href="#7-更改yum配置，因为其要用到python2才能执行，否则会导致yum不能正常使用（不管安装-python3的那个版本，都必须要做的）" class="headerlink" title="7. 更改yum配置，因为其要用到python2才能执行，否则会导致yum不能正常使用（不管安装 python3的那个版本，都必须要做的）"></a>7. 更改yum配置，因为其要用到python2才能执行，否则会导致yum不能正常使用（不管安装 python3的那个版本，都必须要做的）</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /usr/bin/yum </span><br><span class="line">把 #! /usr/bin/python 修改为 #! /usr/bin/python2 </span><br><span class="line">vi /usr/libexec/urlgrabber-ext-down </span><br><span class="line">把 #! /usr/bin/python 修改为 #! /usr/bin/python2</span><br></pre></td></tr></table></figure><h4 id="8-virtualenv安装"><a href="#8-virtualenv安装" class="headerlink" title="8. virtualenv安装"></a><strong>8. virtualenv安装</strong></h4><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# pip3 install virtualenv</span><br><span class="line">pip is configured <span class="keyword">with</span> locations that <span class="built_in">require</span> TLS/SSL, however the ssl <span class="built_in">module</span> <span class="keyword">in</span> Python is not available.</span><br><span class="line">Collecting virtualenv</span><br></pre></td></tr></table></figure><p>如果出现以上信息，说明要安装openssl </p><p>yum install -y openssl openssl-devel</p><p>yum clean all</p><p>再次执行命令pip3 install virtualenv，发现还是同样的问题。</p><p>查阅资料发现，在./configure过程中，如果没有加上–with-ssl参数时，默认安装的软件涉及到ssl的功能不可用，刚好pip3过程需要ssl模块，而由于没有指定，所以该功能不可用。</p><p>解决办法是重新对python3.6进行编译安装，用一下过程来实现编译安装:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/src/Python-<span class="number">3.6</span><span class="number">.5</span>/</span><br><span class="line">./configure --<span class="keyword">with</span>-ssl --prefix=<span class="regexp">/usr/</span>local/python3</span><br><span class="line">make -j4 &amp;&amp; make install</span><br><span class="line">cd /root/</span><br></pre></td></tr></table></figure><p>再次执行安装命令，就可以了</p><p>pip3 install virtualenv</p><p>提示以下信息：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">You are using pip version <span class="number">9.0</span><span class="number">.3</span>, however version <span class="number">10.0</span><span class="number">.1</span> is available.</span><br><span class="line">You should consider upgrading via the <span class="string">&#x27;pip install --upgrade pip&#x27;</span> command.</span><br></pre></td></tr></table></figure><p>升级pip</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# pip3 install --upgrade pip</span><br><span class="line">Collecting pip</span><br><span class="line">  Downloading https:<span class="comment">//files.pythonhosted.org/packages/0f/74/ecd13431bcc456ed390b44c8a6e917c1820365cbebcb6a8974d1cd045ab4/pip-10.0.1-py2.py3-none-any.whl (1.3MB)</span></span><br><span class="line">    <span class="number">100</span>% |████████████████████████████████| <span class="number">1.</span>3MB 593kB/s</span><br><span class="line">Installing collected packages: pip</span><br><span class="line">  Found existing installation: pip <span class="number">9.0</span><span class="number">.3</span></span><br><span class="line">    Uninstalling pip-<span class="number">9.0</span><span class="number">.3</span>:</span><br><span class="line">      Successfully uninstalled pip-<span class="number">9.0</span><span class="number">.3</span></span><br><span class="line">Successfully installed pip-<span class="number">10.0</span><span class="number">.1</span></span><br></pre></td></tr></table></figure><h4 id="9-安装virtualenvwrapper"><a href="#9-安装virtualenvwrapper" class="headerlink" title="9. 安装virtualenvwrapper"></a><strong>9. 安装virtualenvwrapper</strong></h4><p>pip3 install virtualenvwrapper</p><p>linux还需要安装一个包，否则找不到virtualenvwrapper.sh文件</p><p>yum install python-virtualenv -y</p><p>首选找到对应的virtualenvwrapper.sh文件的路径：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# find / -name virtualenvwrapper.sh</span><br><span class="line">/usr/local/python3/bin/virtualenvwrapper.sh</span><br></pre></td></tr></table></figure><p>创建虚拟环境目录 </p><p>mkdir /virtualenvs</p><p>找到路径之后在/etc/profile中进行配置：</p><p>vim /etc/profile</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> WORKON_HOME=/virtualenvs</span><br><span class="line">source /usr/local/python3/bin/virtualenvwrapper.sh</span><br></pre></td></tr></table></figure><p>第一行：virtualenvwrapper存放虚拟环境目录</p><p>第二行：virtrualenvwrapper会安装到python的bin目录下，所以该路径是python安装目录下</p><p>读入配置文件，立即生效</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost virtualenvs]# source /etc/profile</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/premkproject</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/postmkproject</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/initialize</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/premkvirtualenv</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/postmkvirtualenv</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/prermvirtualenv</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/postrmvirtualenv</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/predeactivate</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/postdeactivate</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/preactivate</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/postactivate</span><br><span class="line">virtualenvwrapper.user_scripts creating /virtualenvs/get_env_details</span><br></pre></td></tr></table></figure><p>如果运行报错，如下：</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/python: No <span class="built_in">module</span> named virtualenvwrapper</span><br></pre></td></tr></table></figure><p>请确保python版本为3.6。如何将系统的python版本替换为3.6，请参考文章 </p><p><a href="http://www.py3study.com/Article/details/id/320.html">http://www.py3study.com/Article/details/id/320.html</a> </p><h4 id="10-使用虚拟环境"><a href="#10-使用虚拟环境" class="headerlink" title="10.使用虚拟环境"></a>10.使用虚拟环境</h4><ul><li><p>提示：如果不指定python版本，默认安装的是python2的虚拟环境</p></li><li><p>在python2中，创建虚拟环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkvirtualenv 虚拟环境名称</span><br><span class="line">例 ：</span><br><span class="line">mkvirtualenv py_django</span><br></pre></td></tr></table></figure></li><li><p>在python3中，创建虚拟环境</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkvirtualenv -p python3 虚拟环境名称</span><br><span class="line">例 ：</span><br><span class="line">mkvirtualenv -p python3 py3_django</span><br></pre></td></tr></table></figure></li><li><p>查看虚拟环境的命令 :</p></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">workon</span><br></pre></td></tr></table></figure><ul><li>退出虚拟环境的命令 :</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deactivate</span><br></pre></td></tr></table></figure><ul><li>删除虚拟环境的命令 :</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rmvirtualenv 虚拟环境名称</span><br><span class="line"></span><br><span class="line">例 ：删除虚拟环境py3_django</span><br><span class="line"></span><br><span class="line">先退出：deactivate</span><br><span class="line">再删除：rmvirtualenv py3_django</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> python基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python3 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
